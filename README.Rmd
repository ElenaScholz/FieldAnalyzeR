---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# fastR

<!-- badges: start -->

<!-- badges: end -->

## The goal of PACKAGENAME is ...

The primary objective of the provided package is to:

-   Simplify the preprocessing steps for in-situ temperature data time series selection.

-   Enable users to focus on data analysis.

-   Facilitate access to AppEEARS data within R, particularly referencing datasets derived from remote sensing applications.

    <!-- -->

## Installation

You can install the development version of PACKAGENAME like so:

``` r
# devtools::install_github("ElenaScholz/PACKAGENAME")
```

## Prerequisites

-   InSitu or other Time Series Data with the Informations to be analysed (Should look similar to the provided datasets)
-   An account to NASA's EARTHDATA portal ([Create a new Account](http://urs.earthdata.nasa.gov) ) to download reference data

## Workflow

1.  Data Analysis of field Temperature Data

    1.  Read in data and reorganize it into a list of data frames

    2.  Improve the column readability by renaming them

    3.  Enhance data structure by mutating the date/time column

    4.  Aggregate data frames by Year, Month, Day or Season

2.  Exploration of the Datasets

    1.  Create simple plots to gain an overview of the logger data.

    2.  Utilize the 'show_appeears_products' function to explore all available AppEEARS products.

3.  Retrive Reference Data

    1.  Authenticate with AppEEARS using 'appeears_login' to obtain a token

    2.  Submit tasks to download reference data and filter all products by the topics for your analysis

4.  Data Download and Processing

    1.  After the task submission is done you'll receive an Email and you can start the data download

5.  Integration and Visualization

    1.  Apply similar pre-processing steps (1.1 -1.4) to the downloaded reference datasets

    2.  Plot logger data and reference datasets together

## Example

### Preprocessing

This codesnippet shows the workflow for preparing the data for the analysis

```{r example}
library(fastR)


input_directory <- system.file("extdata/Logger/", package = "fastR")

### 1. Read in the Datasets with the function "read_data"
  # for the given example datasets, the ID can be added from the filename. 
raw_data <- read_data(input_directory = input_directory, csv_sep = ",",
                      csv_comment_character = "#",
                      add_ID_from_filename = TRUE,
                      index_id = c(0, 6)
) 

  # take a look at the structure of raw_data and one dataset
str(raw_data)

### 2. Rename the columns to make the Analysis easier

sample_data <- rename_columns(raw_data, 
                               rename_map = list(Number = "No", 
                                                 Logger_ID = "Logger.ID", 
                                                 Time = "Time", 
                                                 Temperature_C  = "X1.oC",
                                                 Battery_Voltage = "HK.Bat.V"))

### 3. Date Conversion 
  # for further analysis it is necessary to have the Date converted to the type         "Date". Therefore check if the format of the Time Column. 


sample_data <- mutate_dates(sample_data, time_column = "Time",
                            time_format = "%d.%m.%Y %H:%M")

head(sample_data)

```

### Get reference data

```{r }
### 1. Load in Coordinate file and examine the structure
coordinates <- read_data(input_directory = system.file("extdata/Coordinates/", package = "fastR"), add_ID_from_filename = FALSE)
  # Look at the dataset - it contains information about x and y            position 
str(coordinates)
head(coordinates)


### 2. create a spatial dataset, that converts the original                   coordinatesystem into longitude and latitude 

coordinates_transformed <- make_spatial_data(coordinates, coordinate_column = c("X", "Y"), data_crs_original = coordinates$EPSG, transformed_crs = "+proj=longlat +datum=WGS84", logger_id_column = "Logger.ID")

  # the new dataset is a list containing an sf-object and a dataframe      for the download
class(coordinates_transformed)

appeears_coordinates <- coordinates_transformed$coordinates_df

### 3. Login to AppEEARS and generate a token 

token <- appeears_login()

### 4. get an overview over all appeears products and choose which reference data you want to download

products <- show_appeears_products()

### 5. submit a processing task for the reference data
    # in this case reference data for landsurface temperature (LST) and        Snow Cover is chosen (NDSI)
start_date <- min(sample_data$Date)
end_date <- max(sample_data$Date)

lst_submission <- submit_processing_task(task_name = "example_lst", products_df = products, topic_filter = "LST", token = token, start_date = start_date, end_date = end_date) 

ndsi_submission <- submit_processing_task(task_name = "example_lst", products_df = products, topic_filter = "NDSI", token = token, start_date = start_date, end_date = end_date) 
### 6. download the reference data

```

You'll still need to render `README.Rmd` regularly, to keep `README.md` up-to-date. `devtools::build_readme()` is handy for this.

You can also embed plots, for example:

```{r pressure, echo = FALSE}
plot(pressure)
```

In that case, don't forget to commit and push the resulting figure files, so they display on GitHub and CRAN.
